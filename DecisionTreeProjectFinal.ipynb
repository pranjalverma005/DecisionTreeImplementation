{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "iris=datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "# SEAPRATE X and Y FROM DATA\n",
    "x_iris=iris.data\n",
    "y_iris=iris.target\n",
    "\n",
    "x=np.array([np.array(xi) for xi in x_iris])\n",
    "y=np.array(y_iris)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FEATURE NAMES\n",
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0\n",
      "count of  0 ( setosa ) = 50\n",
      "count of  1 ( versicolor ) = 50\n",
      "count of  2 ( virginica ) = 50\n",
      "current entropy= 1.0986122886681096\n",
      "Splitting on feature petal length (cm) with gain ratio 1.0\n",
      "\n",
      "level 1\n",
      "count of  0 ( setosa ) = 50\n",
      "current entropy=0\n",
      "Reached leaf node\n",
      "\n",
      "level 1\n",
      "count of  1 ( versicolor ) = 50\n",
      "count of  2 ( virginica ) = 50\n",
      "current entropy= 0.6931471805599453\n",
      "Splitting on feature petal width (cm) with gain ratio 0.6621582046482519\n",
      "\n",
      "level 2\n",
      "count of  1 ( versicolor ) = 44\n",
      "count of  2 ( virginica ) = 1\n",
      "current entropy= 0.10656595882801999\n",
      "Splitting on feature sepal length (cm) with gain ratio 0.4166719824070529\n",
      "\n",
      "level 3\n",
      "count of  1 ( versicolor ) = 1\n",
      "count of  2 ( virginica ) = 1\n",
      "current entropy= 0.6931471805599453\n",
      "Splitting on feature sepal width (cm) with gain ratio 1.0\n",
      "\n",
      "level 4\n",
      "count of  1 ( versicolor ) = 1\n",
      "current entropy=0\n",
      "Reached leaf node\n",
      "\n",
      "level 4\n",
      "count of  2 ( virginica ) = 1\n",
      "current entropy=0\n",
      "Reached leaf node\n",
      "\n",
      "level 3\n",
      "count of  1 ( versicolor ) = 43\n",
      "current entropy=0\n",
      "Reached leaf node\n",
      "\n",
      "level 2\n",
      "count of  1 ( versicolor ) = 6\n",
      "count of  2 ( virginica ) = 49\n",
      "current entropy= 0.3446104320908521\n",
      "Splitting on feature sepal length (cm) with gain ratio 0.054638931580928356\n",
      "\n",
      "level 3\n",
      "count of  1 ( versicolor ) = 6\n",
      "count of  2 ( virginica ) = 37\n",
      "current entropy= 0.40411826492136166\n",
      "Splitting on feature sepal width (cm) with gain ratio 0.05194809035157456\n",
      "\n",
      "level 4\n",
      "count of  1 ( versicolor ) = 6\n",
      "count of  2 ( virginica ) = 32\n",
      "current entropy= 0.4361623253853443\n",
      "Reached leaf node\n",
      "\n",
      "level 4\n",
      "count of  2 ( virginica ) = 5\n",
      "current entropy=0\n",
      "Reached leaf node\n",
      "\n",
      "level 3\n",
      "count of  2 ( virginica ) = 12\n",
      "current entropy=0\n",
      "Reached leaf node\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IMPLEMENTATION OF DECISION TREE\n",
    "decisionTree(x,y,iris.feature_names,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTED DECISION TREE FUNCTION\n",
    "def decisionTree(x_train,y_train,features,level):\n",
    "    # BASE CASES 1 or 2\n",
    "    # 1) if the node is PURE\n",
    "    A=set(y_train)\n",
    "    if len(A)==1:\n",
    "        print(\"level\",level)\n",
    "        for i in A:\n",
    "            cls=i\n",
    "        print(\"count of \",cls,\"(\",iris.target_names[cls],\") =\",len(y_train))\n",
    "        print(\"current entropy=0\")\n",
    "        print(\"Reached leaf node\")\n",
    "        print()\n",
    "        return\n",
    "    \n",
    "    # 2) if we run out of features\n",
    "    if len(features)==0:\n",
    "        print(\"level\",level)\n",
    "        y_classes=set(y_train)\n",
    "        for i in y_classes:\n",
    "            print(\"count of \",i,\"(\",iris.target_names[i],\") =\",(y_train==i).sum())\n",
    "        print(\"current entropy=\",entropy(x_train,y_train))\n",
    "        print(\"Reached leaf node\")\n",
    "        print()\n",
    "        return\n",
    "    \n",
    "    # PROCEEDING FOR RECURSIVE CALLS\n",
    "    print(\"level\",level) \n",
    "    y_classes=set(y_train)\n",
    "    for i in y_classes:\n",
    "        print(\"count of \",i,\"(\",iris.target_names[i],\") =\",(y_train==i).sum())\n",
    "    present_entropy=entropy(x_train,y_train)\n",
    "    print(\"current entropy=\",present_entropy)\n",
    "    \n",
    "    # Feature selection in order to maximize gain\n",
    "    max_gain=0\n",
    "    feature_selected=0\n",
    "    feature_value=0\n",
    "    for f in range(len(features)):\n",
    "        # since we have continuous ranged data\n",
    "        # therefore we need the value at which we decided to split\n",
    "        current_gain,value = gain_ratio(x_train,y_train,f,features,iris.feature_names,present_entropy)\n",
    "        if current_gain > max_gain:\n",
    "            max_gain=current_gain\n",
    "            feature_selected=f\n",
    "            feature_value=value\n",
    "    # if there is no gain, reached leaf node \n",
    "    if(max_gain==0):\n",
    "        print(\"Reached leaf node\")\n",
    "        print()\n",
    "        return\n",
    "    \n",
    "    # else splitting will occur\n",
    "    print(\"Splitting on feature\",features[feature_selected],\"with gain ratio\",max_gain)\n",
    "    print()\n",
    "    \n",
    "    # find column in original data for feature_selected\n",
    "    cols=0\n",
    "    for j in range(len(iris.feature_names)):\n",
    "        if iris.feature_names[j] == features[feature_selected]:\n",
    "            cols=j\n",
    "    features.remove(features[feature_selected])\n",
    "    \n",
    "    # Recursive call to the split made on feature_selected\n",
    "    # 1) Calling LEFT NODE by creating new_x_train and new_y_train\n",
    "    criteria = x_train[:,cols] <= feature_value\n",
    "    new_x1,new_y1=x_train[criteria],y_train[criteria]\n",
    "    decisionTree(new_x1,new_y1,features.copy(),level+1)\n",
    "    \n",
    "    # 2) Calling RIGHT NODE by creating new_x_train and new_y_train\n",
    "    criteria = x_train[:,cols] > feature_value\n",
    "    new_x2,new_y2=x_train[criteria],y_train[criteria]\n",
    "    decisionTree(new_x2,new_y2,features.copy(),level+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_ratio(x,y,f,features,original_features,present_entropy):\n",
    "    # find column in original data for feature_selected\n",
    "    cols=0\n",
    "    for j in range(len(original_features)):\n",
    "        if original_features[j] == features[f]:\n",
    "            cols=j\n",
    "        \n",
    "    feature_value=set(x[:,cols])\n",
    "    feature_len=len(feature_value)\n",
    "    feature_value=sorted(feature_value)\n",
    "    \n",
    "    gain_ratio=0\n",
    "    value=0\n",
    "    \n",
    "    # for set of given feature -> iterate over all values -> calculate the maximum gain ratio \n",
    "    for i in range(feature_len - 1):\n",
    "        # choose the midpoint between the 2 values in the feature set\n",
    "        midpoint=(feature_value[i]+feature_value[i+1])/2\n",
    "        \n",
    "        # according to the midpoint -> divide the x_train and y_train -> calculate corresponding entropy\n",
    "        # values corresponding to less than midpoint\n",
    "        criteria = x[:,cols] < midpoint\n",
    "        new_x1,new_y1=x[criteria],y[criteria]\n",
    "        entropy1=entropy(new_x1,new_y1)\n",
    "        \n",
    "        # values corresponding to more than midpoint\n",
    "        criteria = x[:,cols] >= midpoint\n",
    "        new_x2,new_y2=x[criteria],y[criteria]\n",
    "        entropy2=entropy(new_x2,new_y2)\n",
    "        \n",
    "        # calculate final entropy by taking in account how much data each side has\n",
    "        final_entropy=(len(new_y1)/len(y))*entropy1 + (len(new_y2)/len(y))*entropy2\n",
    "        # calculate the split information\n",
    "        split_info= (-len(new_y1)/len(y))*math.log(len(new_y1)/len(y)) - (len(new_y2)/len(y))*math.log(len(new_y2)/len(y))\n",
    "        \n",
    "        # calculate gain ratio\n",
    "        current_gain_ratio=(present_entropy-final_entropy)/split_info\n",
    "        \n",
    "        # maximize gain ratio for a value in feature\n",
    "        if(current_gain_ratio > gain_ratio):\n",
    "            gain_ratio=current_gain_ratio\n",
    "            value=midpoint\n",
    "            \n",
    "    return gain_ratio,value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(x,y):\n",
    "    entropy=0\n",
    "    # calculate total size of y_train\n",
    "    total_len=len(y)\n",
    "    y_class=set(y)\n",
    "    \n",
    "    # iterate over each class in y_train\n",
    "    for i in y_class:\n",
    "        # calculate size of selected class\n",
    "        current_class_len=(y==i).sum()\n",
    "        # calculate the probability of a given class \n",
    "        probability_current_class=current_class_len/total_len\n",
    "        # calculate entropy\n",
    "        entropy+= (-probability_current_class)*math.log(probability_current_class)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=4,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf=DecisionTreeClassifier(criterion='entropy', max_depth=4)\n",
    "clf.fit(iris.data,iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9933333333333333"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(iris.data,iris.target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
